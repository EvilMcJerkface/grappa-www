<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.6"/>
<title>Grappa: Grappa Tutorial</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="navtree.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="resize.js"></script>
<script type="text/javascript" src="navtree.js"></script>
<script type="text/javascript">
  $(document).ready(initResizable);
  $(window).load(resizeHeight);
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">Grappa
   &#160;<span id="projectnumber">0.1</span>
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.6 -->
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li class="current"><a href="pages.html"><span>Related&#160;Pages</span></a></li>
      <li><a href="modules.html"><span>Modules</span></a></li>
      <li><a href="namespaces.html"><span>Namespaces</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li><a href="examples.html"><span>Examples</span></a></li>
    </ul>
  </div>
</div><!-- top -->
<div id="side-nav" class="ui-resizable side-nav-resizable">
  <div id="nav-tree">
    <div id="nav-tree-contents">
      <div id="nav-sync" class="sync"></div>
    </div>
  </div>
  <div id="splitbar" style="-moz-user-select:none;" 
       class="ui-resizable-handle">
  </div>
</div>
<script type="text/javascript">
$(document).ready(function(){initNavTree('md__sync_sampa_grappa-b_doc_tutorial_tutorial.html','');});
</script>
<div id="doc-content">
<div class="header">
  <div class="headertitle">
<div class="title"><a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a> Tutorial </div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p>This guide will try to outline the key features of <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a> from an application's perspective at a high level. Detailed description of functionality is left out here; look at the generated Doxygen documentation for details.</p>
<p><em>Warning</em>: <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a> is a research project, and the programming interface for <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a> is still in flux. If you run a problem, please visit our GitHub questions page at <a href="https://github.com/uwsampa/grappa/issues?labels=question,">https://github.com/uwsampa/grappa/issues?labels=question,</a> where you may find a solution or ask for help.</p>
<h2>Section 1: Hello World </h2>
<p>Topics in this section:</p>
<ul>
<li>global-view programming model</li>
<li>what's that in main? (init/run/finalize)</li>
<li><code><a class="el" href="group___collectives.html#gac74a2d7ffb5b36ec6833ebfc54c16841" title="Spawn a private task on each core, block until all complete. ">on_all_cores()</a></code></li>
</ul>
<p>Here is the simplest possible <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a> program:</p>
<p>```cpp /////////////////////////////// // <a class="el" href="hello__world__1_8cpp.html">tutorial/hello_world_1.cpp</a> /////////////////////////////// #include &lt;<a class="el" href="_grappa_8hpp.html">Grappa.hpp</a>&gt; #include &lt;iostream&gt; int <a class="el" href="search2_8cpp.html#a0ddf1224851353fc92bfbff6f499fa97">main(int argc, char *argv[])</a> { Grappa::init(&amp;argc, &amp;argv); <a class="el" href="group___tasking.html#ga1fa04570855ffc3d360a5264f950eb24">Grappa::run</a>([]{ std::cout &lt;&lt; "Hello world from the root task!\n"; }); <a class="el" href="namespace_grappa.html#a661c56b3ed1f67b0ae3228e67a738380" title="Clean up Grappa. ">Grappa::finalize()</a>; } ```</p>
<p>When run with 4 cores (2 nodes, with 2 processes per node) using the Slurm launcher, we get the following output:</p>
<p>```bash </p>
<blockquote class="doxtable">
<p>make tutorial-hello_world_1 bin/grappa_srun &ndash;nnode=2 &ndash;ppn=2 &ndash; tutorial/hello_world_1.exe</p>
<p></p>
</blockquote>
<p>Hello world from the root task! ```</p>
<p><a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a> jobs, like MPI &amp; UPC, are launched with a separate process on each node in the job (known as SPMD &ndash; Single Program Multiple <a class="el" href="struct_data.html">Data</a>). In this case, when Slurm launches the 2-node job, it executes <code>hello_world_1.exe</code> on both nodes. However, <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a> is primarily designed to be a "global view" model, which means that rather than coordinating where all parallel SPMD processes are at and how they divide up the data, the programmer is encouraged to think of the system as a large single shared memory, use data structures that intelligently distribute themselves across the cores, and write parallel loops that get executed in parallel across the whole machine. To that end, the body of the <code><a class="el" href="group___tasking.html#ga1fa04570855ffc3d360a5264f950eb24">Grappa::run()</a></code> call spawns the "root" task on a single core. This task determines the lifetime of the program: when it returns, the <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a> program ends. The intention is that this task will spawn many more tasks and synchronize with all of them and only return when all of the work has been completed. Because it only runs once, the output is only printed once.</p>
<p>Breaking down the rest of what goes on in main:</p>
<ul>
<li><code><a class="el" href="namespace_grappa.html#af7b60a124d5f39fd448e002fa2a3e11f" title="Initialize Grappa. ">init()</a></code>: parses command-line flags and environment variables, and then initializes all of <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a>'s subsystems, such as the communication and threading systems, and global heap allocation.</li>
<li><code><a class="el" href="group___tasking.html#ga1fa04570855ffc3d360a5264f950eb24">run()</a></code>: starts the scheduling (tasking and communication) loop, and on Core 0 only spawns the main <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a> task. All cores stay inside this function until this main task returns, which immediately signals the others to exit the scheduling loop.</li>
<li><code><a class="el" href="namespace_grappa.html#a661c56b3ed1f67b0ae3228e67a738380" title="Clean up Grappa. ">finalize()</a></code>: cleans up the process, closing communication channels and freeing memory; does not return.</li>
</ul>
<h3>SPMD Variant</h3>
<p>To further illustrate the global-view vs. SPMD models, we can add a bit of SPMD execution back into our <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a> program:</p>
<p>```cpp /////////////////////////////// // <a class="el" href="hello__world__2_8cpp.html">tutorial/hello_world_2.cpp</a> /////////////////////////////// #include &lt;<a class="el" href="_grappa_8hpp.html">Grappa.hpp</a>&gt; #include &lt;<a class="el" href="_collective_8hpp.html">Collective.hpp</a>&gt; #include &lt;iostream&gt; int <a class="el" href="search2_8cpp.html#a0ddf1224851353fc92bfbff6f499fa97">main(int argc, char *argv[])</a> { Grappa::init(&amp;argc, &amp;argv); <a class="el" href="group___tasking.html#ga1fa04570855ffc3d360a5264f950eb24">Grappa::run</a>([]{ std::cout &lt;&lt; "Hello world from the root task!\n"; <a class="el" href="group___collectives.html#gac74a2d7ffb5b36ec6833ebfc54c16841" title="Spawn a private task on each core, block until all complete. ">Grappa::on_all_cores</a>([]{ std::cout &lt;&lt; "Hello world from Core " &lt;&lt; <a class="el" href="group___communication.html#ga9d165cbd8cbb54ce375f342722022a91" title="What&#39;s my core ID in this job? ">Grappa::mycore()</a> &lt;&lt; " of " &lt;&lt; <a class="el" href="group___communication.html#gaff2afcac6058e98ea3c6151f2445702b" title="How many cores are there in this job? ">Grappa::cores()</a> &lt;&lt; " (locale " &lt;&lt; <a class="el" href="group___communication.html#ga0de446bfdb2eb6fa603009e110150e3d" title="What&#39;s my shared memory domain ID within this job? ">Grappa::mylocale()</a> &lt;&lt; ")"&lt;&lt; "\n"; }); std::cout &lt;&lt; "Exiting root task.\n"; }); <a class="el" href="namespace_grappa.html#a661c56b3ed1f67b0ae3228e67a738380" title="Clean up Grappa. ">Grappa::finalize()</a>; } ```</p>
<p>This time when run on 4 cores, we may get output like this:</p>
<p>```bash </p>
<blockquote class="doxtable">
<p>make tutorial-hello_world_2 bin/grappa_srun &ndash;nnode=2 &ndash;ppn=2 &ndash; tutorial/hello_world_2.exe</p>
<p></p>
</blockquote>
<p>Hello world from the root task! Hello world from Core 1 of 4 (locale 0) Hello world from Core 0 of 4 (locale 0) Hello world from Core 3 of 4 (locale 1) Hello world from Core 2 of 4 (locale 1) Exiting root task. ```</p>
<p>This should look familiar to the MPI programmer. The call to <code><a class="el" href="group___collectives.html#gac74a2d7ffb5b36ec6833ebfc54c16841" title="Spawn a private task on each core, block until all complete. ">on_all_cores()</a></code> spawns a task on each core and then suspends the calling task (the root task in this case) until all those tasks have completed. Therefore, we do not see the "Exiting" message until after all of the "Hello"s, though those may arrive in any order.</p>
<p>This also introduces the API for identifying cores. In <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a>, each core is a separate destination. The physical node on which a core resides is referred to as its "locale" (borrowed from Chapel's terminology). The next section will refer to these functions more when dealing with how memory is partitioned among cores.</p>
<h2>Section 2: Global memory </h2>
<p>In <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a>, all memory on all cores is addressable by any other core. This is done using the <code><a class="el" href="class_global_address.html" title="Global address class. ">GlobalAddress</a>&lt;T&gt;</code> template class. In the spirit of other Partitioned Global Address Space (PGAS) languages and runtimes, <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a> provides mechanisms for easily distributing data across the various memories of nodes in a cluster.</p>
<p>This section will:</p>
<ul>
<li>Describe global addressing, partitioning and the global heap</li>
<li>Introduce the various ways of addressing and allocating global memory</li>
</ul>
<h3>Global Addresses &amp; Allocators</h3>
<p>Global addresses fall into one of three different categories:</p>
<ul>
<li><b>2D addresses</b>: can address any memory on a core, including memory on stacks of currently-executing tasks, or memory allocated using <code>malloc</code> or <code>new</code></li>
<li><b>Linear addresses</b>: cyclicly partitioned among cores, allocated from the global heap using <code>global_alloc&lt;T&gt;()</code>, freed with <code><a class="el" href="namespace_grappa.html#a0b29dfddf8e0c5d3c912565a0b1db6d0" title="Free memory allocated from global shared heap. ">global_free()</a></code></li>
<li><b>Symmetric addresses</b>: same address valid on all cores, allocated from the global heap using <code>symmetric_global_alloc&lt;T&gt;()</code>, freed with <code><a class="el" href="namespace_grappa.html#a0b29dfddf8e0c5d3c912565a0b1db6d0" title="Free memory allocated from global shared heap. ">global_free()</a></code></li>
</ul>
<p>Pretty much anywhere in the <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a> API that takes a <a class="el" href="class_global_address.html" title="Global address class. ">GlobalAddress</a> will work with any of these types (they are all the same class), but they will have different behavior in terms of how they map to cores.</p>
<h4>2D Addresses</h4>
<p>The first, 2D addresses, are the simplest. They represent just the pairing of a pointer, and the Core where that pointer is valid. These can be created using the call <code><a class="el" href="group___memory.html#ga55c522130968fe1140cb8505dc9eac1a" title="return a 2d global pointer to a local pointer on a particular core ">make_global()</a></code>, and will typically reference either data on a worker's execution stack, or something allocated from the core-private heap (using <code>new</code>). Consider the following example (note: the rest of <code><a class="el" href="_context_switch_rate__bench_8cpp.html#a0ddf1224851353fc92bfbff6f499fa97">main()</a></code> has been left out, see the full source at <code>tutorial/addressing.cpp</code>):</p>
<p>```cpp long count = 0; GlobalAddress&lt;long&gt; g_count = make_global( &amp;count ); LOG(INFO) &lt;&lt; "global address -- valid on core:" &lt;&lt; g_count.core() &lt;&lt; ", pointer:" &lt;&lt; g_count.pointer(); ```</p>
<p>The call to <code><a class="el" href="group___memory.html#ga55c522130968fe1140cb8505dc9eac1a" title="return a 2d global pointer to a local pointer on a particular core ">make_global()</a></code> creates a new global address with the given pointer to the <code>count</code> variable on this task's stack and encodes the core where it is valid (<code><a class="el" href="group___communication.html#ga9d165cbd8cbb54ce375f342722022a91" title="What&#39;s my core ID in this job? ">Grappa::mycore()</a></code>) with it.</p>
<h4>Linear addresses</h4>
<p>A portion of each node's global memory is set aside specifically for the <em>global heap</em>. This amount is determined by the command-line flag <code>--global_heap_fraction</code> (which defaults to 0.5). Allocations from this heap are distributed in a block-cyclic round-robin fashion across cores. Each core has a region of memory set aside for the global heap, with a heap offset pointer specific for the core. To allocate from this heap, the following functions are used:</p>
<ul>
<li><code><a class="el" href="class_global_address.html" title="Global address class. ">GlobalAddress</a>&lt;T&gt; <a class="el" href="namespace_grappa.html#af74993efb7bbea33d4e7e79953502cf1" title="Allocate bytes from the global shared heap. ">Grappa::global_alloc</a>&lt;T&gt;(size_t num_elements)</code>: allocates space for <code>num_elements</code> of size <code>T</code>, starting on some arbitrary core</li>
<li><code>void <a class="el" href="namespace_grappa.html#a0b29dfddf8e0c5d3c912565a0b1db6d0" title="Free memory allocated from global shared heap. ">Grappa::global_free</a>(<a class="el" href="class_global_address.html" title="Global address class. ">GlobalAddress</a>&lt;T&gt;)</code>: frees global heap allocation</li>
</ul>
<p><em>Linear</em> global addresses, returned by the global heap allocation function, are specifically designed for working with allocations from the global heap. Arithmetic on linear addresses will resolve to the correct core given the heap's block size, and on that core, will use the core's heap offset pointer to give the correct pointer to data.</p>
<p>We will illustrate this with an example. Here we allocate an array of 48 elements, which, due to <code>block_size</code> being set to 64 bytes, allocates 8 to each core before moving on to the next core.</p>
<p>```cpp auto array = global_alloc&lt;long&gt;(48); for (auto i=0; i&lt;48; i++) { std::cout &lt;&lt; "[" &lt;&lt; i &lt;&lt; ": core " &lt;&lt; (array+i).core() &lt;&lt; "] "; } std::cout &lt;&lt; "\n"; ```</p>
<p>```bash </p>
<blockquote class="doxtable">
<p>grappa_srun &ndash;nnode=2 &ndash;ppn=2 &ndash; tutorial/addressing_linear.exe</p>
<p></p>
</blockquote>
<p>[0: core 0] [1: core 0] [2: core 0] [3: core 0] [4: core 0] [5: core 0] [6: core 0] [7: core 0] [8: core 1] [9: core 1] [10: core 1] [11: core 1] [12: core 1] [13: core 1] [14: core 1] [15: core 1] [16: core 2] [17: core 2] [18: core 2] [19: core 2] [20: core 2] [21: core 2] [22: core 2] [23: core 2] [24: core 3] [25: core 3] [26: core 3] [27: core 3] [28: core 3] [29: core 3] [30: core 3] [31: core 3] [32: core 0] [33: core 0] [34: core 0] [35: core 0] [36: core 0] [37: core 0] [38: core 0] [39: core 0] [40: core 1] [41: core 1] [42: core 1] [43: core 1] [44: core 1] [45: core 1] [46: core 1] [47: core 1] ```</p>
<h4>Symmetric addresses</h4>
<p>Another kind of allocation that is possible is a "symmetric" allocation. This allocates a copy of the struct on each core from the global heap, returning a <a class="el" href="class_global_address.html" title="Global address class. ">GlobalAddress</a> that is valid (and has the same offset, hence "symmetric") on every core. Symmetric global addresses are typically for data structures where it is desired to have something to refer to no matter which core one is on. <em>Due to limitations right now, you must pad the struct to be a multiple of the block size. This can be done using the macro: <code>GRAPPA_BLOCK_ALIGNED</code></em>.</p>
<p>Below is an example of allocating a struct on all cores:</p>
<p>```cpp ///////////////////////////////////// // <a class="el" href="addressing__symmetric_8cpp.html">tutorial/addressing_symmetric.cpp</a> ///////////////////////////////////// #include &lt;<a class="el" href="_grappa_8hpp.html">Grappa.hpp</a>&gt; #include &lt;<a class="el" href="_collective_8hpp.html">Collective.hpp</a>&gt; #include &lt;<a class="el" href="_global_allocator_8hpp.html">GlobalAllocator.hpp</a>&gt;</p>
<p>using namespace <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a>;</p>
<p>struct <a class="el" href="struct_data.html">Data</a> { size_t N; long *buffer;</p>
<p>void init(size_t N) { this-&gt;N = N; this-&gt;buffer = new long[32]; } } GRAPPA_BLOCK_ALIGNED;</p>
<p>int <a class="el" href="search2_8cpp.html#a0ddf1224851353fc92bfbff6f499fa97">main(int argc, char *argv[])</a> { init(&amp;argc, &amp;argv); run([]{ // allocate a copy of <a class="el" href="struct_data.html">Data</a> on every core out of the global heap GlobalAddress&lt;Data&gt; d = <a class="el" href="namespace_grappa.html#a4d7e988484354f73edd1d56c5c07f7e7" title="Allocate space for a T at the same localizable global address on all cores (must currently round up t...">symmetric_global_alloc&lt; Data &gt;()</a>;</p>
<p>on_all_cores([d]{ // use <code>-&gt;</code> overload to get pointer to local copy to call the method on d-&gt;init(1024); });</p>
<p>// now we have a local copy of the struct available anywhere on_all_cores([d]{ d-&gt;buffer[0] = d-&gt;N; }); }); <a class="el" href="namespace_grappa.html#a661c56b3ed1f67b0ae3228e67a738380" title="Clean up Grappa. ">finalize()</a>; } ```</p>
<p>In this example, we want a copy of the <a class="el" href="struct_data.html">Data</a> struct on each core so tasks can access a local version no matter where they run. After calling <code><a class="el" href="namespace_grappa.html#a4d7e988484354f73edd1d56c5c07f7e7" title="Allocate space for a T at the same localizable global address on all cores (must currently round up t...">symmetric_global_alloc()</a></code> the structs have not been initialized, so we must call <code><a class="el" href="namespace_grappa.html#af7b60a124d5f39fd448e002fa2a3e11f" title="Initialize Grappa. ">init()</a></code> on each copy. Here we use the <code>-&gt;</code> operator overload for symmetric addresses to get the pointer to the local copy and call the method on it. Finally, we can now reference that allocated struct by just passing around the <a class="el" href="class_global_address.html" title="Global address class. ">GlobalAddress</a> to the symmetric allocation.</p>
<h2>Section 3: Delegate operations </h2>
<p>One of the core tenets of <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a>'s programming model is the idea of <em>delegating</em> remote memory accesses to the core that owns the memory. By ensuring that only one core ever accesses each piece of memory, we can provide strong atomicity guarantees to <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a> programs that even single-node multithreaded applications typically do not. It also allows us to more confidently play with the memory hierarchy to optimize for low-locality access by careful management of caches and pre-fetching. Finally, delegating work to where the data is provides several advantages: it allows work to be distributed evenly across the machine and in some cases minimizes communication, provided the data is laid out well.</p>
<p>If you want data that resides on another core, you will likely use a delegate operation of some sort to accomplish your work. Delegates send a request to another core and block the caller; eventually, the other core executes the request and sends a response which wakes the caller and may return a value. By blocking the caller and executing the delegate atomically, <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a> provides sequentially consistent access to distributed memory (more about this in some of our papers).</p>
<p>The most basic unit of delegates is <code><a class="el" href="group___delegates.html#ga9314cb77365ab51d6fe4772c611921bc">call()</a></code>, which has a couple different forms for convenience:</p>
<p>```cpp T call(Core dest, []() -&gt; T { /* (on 'dest') }) U call(GlobalAddress&lt;T&gt; gp, [](T <em>p) -&gt; U { /</em> (on gp.core(), p = gp.pointer()) }) ```</p>
<p>The provided lambda is executed on the specified core, blocking the caller. The return value of the lambda is returned to the caller when it is woken. The only restriction on code in the delegate is that is <em>must not suspend or yield to the scheduler</em>. This is to ensure that communication workers do not get stuck. An assertion will fail if this is violated. If you need to do blocking remotely, you can use <code><a class="el" href="group___delegates.html#gac3b9a55615b0754717c2b20ef63044c2" title="Synchronizing remote private task spawn. ">spawnRemote()</a></code> to create a task on the remote core, which will be able to do blocking operations. Alternatively, there is another form of <code>delegate::call</code> that takes a Mutex as an argument. See the Doxygen documentation for more details.</p>
<p>Some simple helper delegates for common operations are provided; they are implemented using the above generic delegates.</p>
<ul>
<li><code>T read(<a class="el" href="class_global_address.html" title="Global address class. ">GlobalAddress</a>&lt;T&gt;)</code>: reads memory at that address</li>
<li><code>void write(<a class="el" href="class_global_address.html" title="Global address class. ">GlobalAddress</a>&lt;T&gt;, T val)</code>: writes a value into memory at that address</li>
<li><code>T fetch_and_add(<a class="el" href="class_global_address.html" title="Global address class. ">GlobalAddress</a>&lt;T&gt;, T inc)</code>: increments the value in memory and returns the previous value</li>
<li><code>bool compare_and_swap(<a class="el" href="class_global_address.html" title="Global address class. ">GlobalAddress</a>&lt;T&gt;, T cmp, T val)</code>: if value is equal to <code>cmp</code>, sets it to <code>val</code> and returns <code>true</code>, else returns <code>false</code></li>
</ul>
<p>Delegate operations have some template parameters that allow their behavior to be customized, such as making them "asynchronous". This will be covered in the "Intermediate" part of the tutorial.</p>
<p>The following example demonstrates using delegates to access memory in the global heap, distributed among all the cores.</p>
<p>```cpp size_t N = 50; GlobalAddress&lt;long&gt; array = global_alloc&lt;long&gt;(N);</p>
<p>// simple global write for (size_t i = 0; i &lt; N; i++) { delegate::write( array+i, i ); }</p>
<p>for (size_t i = 0; i &lt; N; i += 10) { // boring remote read long value = delegate::read( array+i ); std::cout &lt;&lt; "[" &lt;&lt; i &lt;&lt; "] = " &lt;&lt; value;</p>
<p>// do some arbitrary computation double v = delegate::call(array+i, [](long *a){ return tan(*a); }); std::cout &lt;&lt; ", tan = " &lt;&lt; v &lt;&lt; std::endl; } ```</p>
<p>```bash </p>
<blockquote class="doxtable">
<p>make tutorial-delegates &amp;&amp; bin/grappa_srun &ndash;nnode=2 &ndash;ppn=2 &ndash; tutorial/delegates.exe</p>
<p></p>
</blockquote>
<p>0: [0] = 0, tan = 0 0: [10] = 10, tan = 0.648361 0: [20] = 20, tan = 2.23716 0: [30] = 30, tan = -6.40533 0: [40] = 40, tan = -1.11721 ```</p>
<p>This is still using the single root task to do all the work, so it is all still serial. The next section will cover how to spawn lots of parallel work efficiently.</p>
<h2>Section 4: Tasking </h2>
<p><b>Be aware:</b> Terminology about threading is very overloaded; everyone means something different when talking about them. In <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a>, we try to avoid using the term "thread". Instead, we have <em>tasks</em> which are a (typically small) unit of work and <em>workers</em> which execute tasks. This is explained in more detail in this section.</p>
<p>The most basic unit of parallelism in <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a> is the <em>task</em>. A <em>task</em> is a unit of work that has some state and some code to run; this is typically specified using a C++11 <em>lambda</em>, but can also be specified with a more traditional C++ <em>functor</em> (a class with <code>operator()</code> overloaded). Tasks are not run immediately after they are created; instead, they go on a queue of un-started tasks.</p>
<p><em>Workers</em> are lightweight user-level threads that are <em>cooperatively scheduled</em> (sometimes also known as <em>fibers</em>), which means that they run uninterrupted on a core until they choose to <em>yield</em> or <em>suspend</em>. A worker takes an un-started task off the queue, executes it to completion, suspending or yielding as the task specifies. When the task finishes (returns from the lambda/functor), the worker goes off to find another task to execute.</p>
<p>One of the benefits of our approach to multithreading is that within a Core, tasks and active messages are multiplexed on a single core, so only one will ever be running at a given time, and context switches happen only at known places: calls to synchronization and remote delegate operations. If, on the other hand, a region of code only references data local to a core, atomicity is guaranteed without any explicit synchronization needed.</p>
<h3>Bound/unbound tasks</h3>
<p>By default, tasks are "bound" to the core they are created on. That is, they will stay on their local task queue until they are eventually picked up by a worker on that core. Tasks can also be spawned "unbound", which puts them into a different queue. These tasks are load-balanced across all the cores in the cluster (using work-stealing). Therefore these "unbound" tasks have to handle being run from any core, and must be sure to fetch any additional data they need.</p>
<h3>Spawning tasks</h3>
<p>The <code>spawn</code> command creates a new task and automatically adds it to the queue of tasks which Workers pull from:</p>
<p>```cpp // 'bound' task, will run on the same core spawn([]{ /* task code });</p>
<p>// 'unbound' task, may be load-balanced to any core spawn&lt;unbound&gt;([]{ /* task code }); ```</p>
<h3>Task synchronization</h3>
<p>Spawns happen "asynchronously". That is, the task that called "spawn" continues right away, not waiting for the spawned task to get run. To ensure that a task has been executed before performing some operation, it must be synchronized with explicitly.</p>
<p><a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a> provides a number of different options for synchronization, which we will cover more fully in a later section. Tasks may use any of the synchronization primitives directly, but here we will demonstrate just one way: using a <code>CompletionEvent</code>. Remember that the <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a> program terminates when the "run" task completes, so if that task does not block on spawned tasks completing, then they may not execute.</p>
<p>```cpp run([]{ LOG(INFO) &lt;&lt; "'main' task started";</p>
<p>CompletionEvent joiner;</p>
<p>spawn(&amp;joiner, []{ LOG(INFO) &lt;&lt; "task ran on " &lt;&lt; <a class="el" href="group___communication.html#ga9d165cbd8cbb54ce375f342722022a91" title="What&#39;s my core ID in this job? ">mycore()</a>; });</p>
<p>spawn&lt;unbound&gt;(&amp;joiner, []{ LOG(INFO) &lt;&lt; "task ran on " &lt;&lt; <a class="el" href="group___communication.html#ga9d165cbd8cbb54ce375f342722022a91" title="What&#39;s my core ID in this job? ">mycore()</a>; });</p>
<p><a class="el" href="group___synchronization.html#ga0e49ee2c2ae50e793e427b0fcfa842f9" title="Proxy for remote ConditionVariable manipulation. ">joiner.wait()</a>;</p>
<p>LOG(INFO) &lt;&lt; "all tasks completed, exiting..."; }); ```</p>
<p>Possible output:</p>
<p>``` 0: 'main' task started 1: task ran on 1 0: task ran on 0 0: all tasks completed, exiting ```</p>
<p>When the <em>main</em> task spawned the two other tasks, it passed in a pointer to the CompletionEvent it made. This caused "spawn" to enroll the tasks with the CompletionEvent, and causes them to automatically signal their "completion" when they finish. This allows the main task, after enrolling the two spawned tasks, to suspend, until the last enrolled task (could be either of them) signals completion, which then wakes the main task, and when the main task completes, this signals the <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a> program to come to an end.</p>
<h3>Parallel loops</h3>
<p>Instead of spawning tasks individually, it's almost always better to use a parallel loop of some sort. In addition to just looking better, these parallel loops also go to a significant amount of effort to run efficiently. For instance, they spawn loop iterations recursively until hitting a threshold. This prevents over-filling the task buffers for large loops, prevents excessive task overhead, and improves work-stealing by making it more likely that a single steal will generate significantly more work. They also spread out spawns across cores in the system, and when iterating over a region of linear global addresses, schedule the iterations to be as close to their data as possible.</p>
<p>The basic parallel loop is <code><a class="el" href="namespace_grappa.html#a1c507e85501aa5a02cee9c2dae4912fe">forall()</a></code>. The iteration range can be specified in a few different ways:</p>
<ul>
<li><code>forall(startIndex, nIterations, [](int64_t i){ })</code>: Specify a start index and a number of iterations. The iterations will be split evenly across all the cores, broken up into evenly-sized blocks.</li>
<li><code>forall(address, nElements, [](T&amp; e){ })</code>: Specify a linear address (start of an allocation from the global heap, for instance), and a number of elements. Iterations will be executed <em>at the core where the corresponding element lives</em>, and the lambda will be passed a simple reference to the element.</li>
<li><code>forall_here(startIndex, nIterations, [](int64_t i){ })</code>: Like the above forall, except instead of spreading iterations across all cores, it spawns them all locally (though if spawning unbound tasks, they may be moved).</li>
</ul>
<p>Each <code>forall</code> loop accepts different forms of lambda, allowing for a bit more control. For instance, a <code>forall</code> over elements in an array of <code>double</code>s could be invoked:</p>
<p>```cpp // just a reference to the element forall(array, 100, [](double&amp; e){ /* ... });</p>
<p>// reference + the absolute index of the element forall(array, 100, [](int64_t i, double&amp; e){ /* ... });</p>
<p>// a range of 'n' iterations, starting with 'start', and a pointer to the first one. // (not often used outside of library implementations) forall(array, 100, [](int64_t start, int64_t n, double * e){ /* ... }); ```</p>
<p>Finally, each of the parallel loops allows specifying template parameters which affect how they run. The most notable of which are:</p>
<ul>
<li>TaskMode (Bound/Unbound): Just as <code>spawn</code> can optionally spawn "unbound" tasks which are load-balanced, parallel loops take the same parameter and use it when spawning tasks.</li>
<li>SyncMode (Blocking/Async): By default, loops block the caller until complete, but one can specify "async" to make the call non-blocking. Most parallel loops use a GlobalCompletionEvent for synchronization. To ensure all "async" loops have run, a task can call <code>wait</code> on the GlobalCompletionEvent used for the loop (specified by another template parameter). By default, all <code>forall</code>s use the same GCE, so best practice is to use <code>async</code> inside of an outer blocking <code>forall</code> (or a call to <code><a class="el" href="namespace_grappa.html#ae60d424e890e6dfc18f880b28b168127">finish()</a></code>), which will ensure all the inner asyncs have finished before continuing.</li>
<li>Threshold: This specifies how many iterations are run serially by a single task. Parallel loops recursively spawn tasks to split the iterations evenly, and stop recursing only when reaching this Threshold. By default, loops use the command-line flag <code>--parallel_loop_threshold</code>, but a different threshold can be specified statically if needed (for instance, if you want to ensure that each iteration gets its own task, you would specify Threshold=1).</li>
<li>GlobalCompletionEvent: this parameter statically specifies a GCE to use for synchronizing the iterations of the loop. Change this only if you cannot use the default GCE for some reason (e.g. you are already using the default GCE concurrently in another context).</li>
</ul>
<p>Because of a bunch of overloaded declarations, these template parameters can be specified in roughly any order, with the rest being left as default. So each of these is valid: <code>forall&lt;unbound&gt;</code>, <code>forall&lt;async&gt;</code>, <code>forall&lt;async,unbound&gt;</code>, <code>forall&lt;unbound,async,1&gt;</code>, <code>forall&lt;&amp;my_gce&gt;</code>, ...</p>
<h2>Section 5: Bringing it all together with GUPS </h2>
<p>We will use a simple benchmark to illustrate the use of parallel loops and delegate operations. "GUPS", which stands for "Giga-Updates Per Second" is a measure and a benchmark for random access rate. The basic premise is to see how quickly you can issue updates to a global array, but the updates are indexed by another array.</p>
<p>```cpp // applications/demos/gups/gups.cpp // make -j TARGET=gups4.exe mpi_run GARGS=" --sizeB=$(( 1 &lt;&lt; 28 )) --loop_threshold=1024" PPN=8 NNODE=12</p>
<p>#include &lt;<a class="el" href="_grappa_8hpp.html">Grappa.hpp</a>&gt;</p>
<p>using namespace <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a>;</p>
<p>// define command-line flags (third-party 'gflags' library) DEFINE_int64( sizeA, 1 &lt;&lt; 30, "Size of array that GUPS increments" ); DEFINE_int64( sizeB, 1 &lt;&lt; 20, "Number of iterations" );</p>
<p>// define custom statistics which are logged by the runtime // (here we're not using these features, just printing them ourselves) GRAPPA_DEFINE_METRIC( SimpleMetric&lt;double&gt;, gups_runtime, 0.0 ); GRAPPA_DEFINE_METRIC( SimpleMetric&lt;double&gt;, gups_throughput, 0.0 );</p>
<p>int <a class="el" href="search2_8cpp.html#a0ddf1224851353fc92bfbff6f499fa97">main(int argc, char * argv[])</a> { init( &amp;argc, &amp;argv ); run([]{</p>
<p>// allocate target array from the global heap auto A = global_alloc&lt;int64_t&gt;(FLAGS_sizeA); // fill the array with all 0's (happens in parallel on all cores) Grappa::memset( A, 0, FLAGS_sizeA );</p>
<p>// allocate another array auto B = global_alloc&lt;int64_t&gt;(FLAGS_sizeB); // initialize the array with random indices forall( B, FLAGS_sizeB, [](int64_t&amp; b) { b = random() % FLAGS_sizeA; });</p>
<p>double start = <a class="el" href="namespace_grappa.html#a10cafc5a51b1eefe09225ca90dd79efb" title="&quot;Universal&quot; wallclock time (works at least for Mac, MTA, and most Linux) ">walltime()</a>;</p>
<p>// GUPS algorithm: // for (int i = 0; i &lt; sizeB; i++) { // A[B[i]] += 1; // }</p>
<p>gups_runtime = <a class="el" href="namespace_grappa.html#a10cafc5a51b1eefe09225ca90dd79efb" title="&quot;Universal&quot; wallclock time (works at least for Mac, MTA, and most Linux) ">walltime()</a> - start; gups_throughput = FLAGS_sizeB / gups_runtime;</p>
<p>LOG(INFO) &lt;&lt; gups_throughput.value() &lt;&lt; " UPS in " &lt;&lt; gups_runtime.value() &lt;&lt; " seconds";</p>
<p>global_free(B); global_free(A);</p>
<p>}); <a class="el" href="namespace_grappa.html#a661c56b3ed1f67b0ae3228e67a738380" title="Clean up Grappa. ">finalize()</a>; } ```</p>
<p>Here we've setup the GUPS benchmark, but left out the main loop that does the updates. We first initialized <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a>, then <code>run</code> the main task. This task will allocate two arrays from the global heap: A, which is the array we'll update, and B, which we fill with random numbers which will index into the A array. Next, we will step through a couple different implementations of the main GUPS loop to demonstrate the features we've discussed so far.</p>
<h3>Simple <code>forall</code></h3>
<p>```cpp // applications/demos/gups/gups1.cpp forall(0, FLAGS_sizeB, [=](int64_t i){ delegate::fetch_and_add( A + delegate::read(B+i), 1); }); ``<code> This implementation uses the form of</code>forall<code>that takes a start index and a number of iterations. Then we specify with a lambda what to do with each iteration, knowing that the parallel loop will instantiate tasks on all the cores to execute the iterations. Note how we're using the implicit capture-by-value form of lambda (</code>[=]<code>). This means that copies of the GlobalAddresses</code>A<code>and</code>B` will be used in the tasks.</p>
<p>We use the simplest delegate operation: <code>read</code> to load the random target index from the B array, and then we use another helper delegate <code>fetch_and_add</code> to increment the indicated element in A.</p>
<p>A minor variant of the above implementation could use <code>forall&lt;unbound&gt;()</code> instead, which would allow iterations to be load-balanced, so that if random variation caused some iterations to take longer, other cores could help out by taking some work themselves.</p>
<h3>Localized <code>forall</code></h3>
<p>The previous implementation is inefficient in one glaringly obvious way: iterations are scheduled blindly onto cores, without considering what data they will touch. We happen to be iterating consecutively over all the elements in the <code>B</code> array. We don't know where each iteration will run, so we must do a remote <code>read</code> to fetch <code>B[i]</code>. Instead, we could use another form of <code>forall</code> to tell the runtime to schedule iterations to line up with elements in the B array:</p>
<p>```cpp // applications/demos/gups/gups2.cpp forall(B, FLAGS_sizeB, [=](int64_t&amp; b){ delegate::fetch_and_add( A + b, 1 ); }); ```</p>
<p>In this version, we no longer need to do a remote reference to get an element of <code>B</code>, each iteration gets a reference to one automatically. We then just do the one remote delegate call to increment that random element in <code>A</code>, wherever it is.</p>
<h3>Asynchronous delegates</h3>
<p>Another wasteful practice in the previous implementations is in using a blocking <code>delegate::fetch_and_add</code> to increment the remote value. As indicated by the name, <code>fetch_and_add</code> doesn't just increment, it also returns the previous value to the caller, which we then promptly ignore. We could save that return trip, and all the nuisance of suspending and waking the calling task, if we invoked a delegate operation that just did the increment asynchronously. Luckily such a call exists:</p>
<p>```cpp // applications/demos/gups/gups3.cpp forall(B, FLAGS_sizeB, [=](int64_t&amp; b){ delegate::increment&lt;async&gt;( A + b, 1); }); ```</p>
<p>This <code><a class="el" href="group___delegates.html#gacad9d4d47cb0c49667bfdd59c604c100">delegate::increment()</a></code> does <em>not</em> return the resulting value, which means we can invoke the <code>async</code> version of it, using the template parameter. This means we issue the increment delegate, and immediately go on to run the next iteration. To ensure all these updates complete before continuing, we must synchronize with these async delegates. By default, specifying <code>&lt;async&gt;</code> enrolls the delegate with the default GlobalCompletionEvent, which is the same GCE that <code>forall</code> uses to synchronize itself, so this "magically" means that the enclosing <code>forall</code> waits until all increments have completed before waking the <em>main</em> task it was called from.</p>
<p>This final version of GUPS is doing an amortized single message per iteration (it takes some messages to setup the parallel loops on all cores, and some combined completion messages to detect when the increments are all completed), which is about as good as we can do. The rest is up to the <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a> runtime to perform these messages as efficiently as possible.</p>
<h3>Custom delegate operations</h3>
<p>Just as an aside, we "lucked out" in our GUPS implementations above in that we had already had a delegate operation to do the increment. If, instead, GUPS called for some other arbitrary operation, it would actually be nearly as simple to implement that just using the generic <code><a class="el" href="group___delegates.html#ga9314cb77365ab51d6fe4772c611921bc">delegate::call()</a></code>:</p>
<p>```cpp // applications/demos/gups/gups4.cpp forall(B, FLAGS_sizeB, [=](int64_t i, int64_t&amp; b){ auto addr = A + b; delegate::call&lt;async&gt;(addr.core(), [addr, i] { *(addr.pointer()) ^= i; }); }); ```</p>
<p>Here, we use the index in the B array to xor the existing value in A. We compute the address of the element (<code>addr</code>), then use it to tell <code>call</code> which core to run on (remember because <code>A</code> is a linear address, the elements are automatically striped across cores). Finally, on the correct core, we ask <code>addr</code> for the local pointer, and use it to update the value. Like <code>delegate::increment</code>, <code>delegate::call</code> can be made "async" in the same way, and it uses the default GCE, too, so we know that all of these will be complete when the <code>forall</code> returns.</p>
<p>Because it is common to do delegates on a single <a class="el" href="class_global_address.html" title="Global address class. ">GlobalAddress</a>, we have an alternate form of <code>call</code> that takes a <a class="el" href="class_global_address.html" title="Global address class. ">GlobalAddress</a>, gets the <code>core()</code> from it, and then passes the <code>pointer()</code> to the provided lambda. Using this version simplifies the implementation nicely:</p>
<p>```cpp forall(B, FLAGS_sizeB, [=](int64_t i, int64_t&amp; b){ delegate::call&lt;async&gt;(A+b, [i](int64_t* a){ *a ^= i; }); }); ```</p>
<h2>Section 6: Nested dynamic parallelism </h2>
<p>We will use a tree search to illustrate the spawning and syncing of an unpredictable number of dynamic tasks.</p>
<p>Let's start with an example problem: We have a tree in global memory. Each node has an id, a color value (0-10), and pointers to its children. The goal of this first version is traverse the tree and count the number of vertices with a given color.</p>
<p>For the sake of brevity, we've hidden away the code to generate the random tree in <code><a class="el" href="tree_8hpp.html">tree.hpp</a></code>, and instead just call <code><a class="el" href="tree_8hpp.html#aca24a3c0ef851eaa0da0cd75cde76eab">create_tree()</a></code>, which returns a <a class="el" href="class_global_address.html" title="Global address class. ">GlobalAddress</a> to the root node.</p>
<p>In order to keep a count of matches found, we're using a little trick: we declare a <code>count</code> variable in the top-level namespace. Remember that <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a> programs are run "SPMD" like MPI programs, so each of these top-level variables are available (and separate) on each core. So, in our <code><a class="el" href="group___collectives.html#gac74a2d7ffb5b36ec6833ebfc54c16841" title="Spawn a private task on each core, block until all complete. ">on_all_cores()</a></code>, we initialize the count to 0, and set the search color (this saves us from having to pass the color around to each task).</p>
<p>```cpp int64_t count; long search_color; GlobalCompletionEvent gce;</p>
<p>int <a class="el" href="search2_8cpp.html#a0ddf1224851353fc92bfbff6f499fa97">main( int argc, char * argv[] )</a> { init( &amp;argc, &amp;argv ); run([]{</p>
<p>size_t num_vertices = 1000;</p>
<p><a class="el" href="class_global_address.html">GlobalAddress&lt;Vertex&gt;</a> root = create_tree(num_vertices);</p>
<p>// initialize all cores on_all_cores([]{ search_color = 7; // arbitrary search count = 0; });</p>
<p>finish&lt;&amp;gce&gt;([=]{ search( root ); });</p>
<p>// compute total count int64_t total = reduce&lt;int64_t,collective_add&lt;int64_t&gt;&gt;(&amp;count);</p>
<p>LOG(INFO) &lt;&lt; "total count: " &lt;&lt; total;</p>
<p>}); <a class="el" href="namespace_grappa.html#a661c56b3ed1f67b0ae3228e67a738380" title="Clean up Grappa. ">finalize()</a>; return 0; } ```</p>
<p>We will delve into the workings of the <code>finish</code> block that calls <code>search</code> recursively next. First, however, let's jump over that and look at what we do after. In the course of the search, we will increment the various copies of <code>count</code> found on each core. To compute the total count, we have to sum up all of them, so we need to invoke a <em>Collective</em> operation. Here we are using a <code>reduce</code>, which is called from one task (<code>main</code> typically), and takes a pointer to a top-level variable. It retrieves the value at that pointer from each core and applies the reduction operation specified by the template parameter. In this case, that is <code>collective_add</code>. After all the results have been summed up, it returns the total value to <code>main</code>.</p>
<p>More of these operations and other ways to invoke collectives can be found under the <code>Collectives</code> tab in the Doxygen documentation, or in <code>Collectives.hpp</code> in the source.</p>
<p>Now we'll explain the workings of the <code>finish</code> block and the search. But first we must introduce GlobalCompletionEvent a bit more fully that we have so far.</p>
<h3>GlobalCompletionEvent</h3>
<p>Up to now, we've treated the GlobalCompletionEvent as a magical way to ensure that everything inside a loop runs before the loop finishes. The truth is, GCE is a very finicky bit of synchronization that allows us to efficiently track many outstanding asynchronous events that may happen on any core in the system and may migrate. GCE implements a "termination detection" algorithm so we can spawn tasks dynamically and still ensure that all of them have completed; we don't have to specify the total number of tasks ahead of time. In addition, to make it efficient, the GCE saves communication by combining multiple "completion" events into a single one.</p>
<p>The downside is that in order to implement these features, GCE's are rather tricky to use. Task spawns, asynchronous delegates, and parallel loops all use the GCE to synchronize. We specify the GCE statically, which means it cannot be created dynamically&mdash;it must be declared as a top-level variable, as in the code example above (<code>gce</code>). It also means that <code>wait</code> calls cannot be nested: a call to wait will no awake until all tasks enrolled with the GCE on all cores have completed, so if the task which calls wait is enrolled, it will deadlock.</p>
<p>The best practice with GCE's is usually to use the default GCE which is specified as a default template parameter for these calls, and then ensure that only the original <code>main</code> task blocks on it. This is what we did in the GUPS examples above.</p>
<p>In our new tree search example, we don't have a top-level parallel loop. Instead, we start with a recursive call to <code>search</code>. Therefore, we use a new function, <code><a class="el" href="namespace_grappa.html#ae60d424e890e6dfc18f880b28b168127">finish()</a></code>. This does nothing more than execute the enclosing lambda, and then call <code><a class="el" href="group___synchronization.html#ga0e49ee2c2ae50e793e427b0fcfa842f9" title="Proxy for remote ConditionVariable manipulation. ">wait()</a></code> on the given GCE. The name harkens to the async/finish-style parallelism espoused by X10 and Habaero Java. Our version is not nearly as sophisticated: it cannot be nested, and enclosing asynchronous calls must ensure that they use the same GCE (if using the default, this is easy).</p>
<p>```cpp finish&lt;&amp;gce&gt;([=]{ search( root ); }); ```</p>
<p>Our <code>search</code> function, which we will show next, recursively creates tasks, enrolling each of them with the same GCE, so that we are guaranteed to have traversed the entire tree before the enclosing <code>finish</code> completes. These tasks, though nested in the sense of our recursive <code>search</code> calls, are not recursively joining, but rather they all join at the one <code>finish</code> call.</p>
<p>```cpp void <a class="el" href="search2_8cpp.html#a02a63a33829d9b2f7a6375259af47fe9">search(GlobalAddress&lt;Vertex&gt; vertex_addr)</a> { // fetch the vertex info <a class="el" href="struct_vertex.html">Vertex</a> v = delegate::read(vertex_addr);</p>
<p>// check the color if (v.color == search_color) count++;</p>
<p>// search children <a class="el" href="class_global_address.html">GlobalAddress&lt;Vertex&gt;</a> children = v.first_child; forall_here&lt;unbound,async,&amp;gce&gt;(0, v.num_children, [children](int64_t i){ search(children+i); }); } ```</p>
<p>The function above takes a <a class="el" href="class_global_address.html" title="Global address class. ">GlobalAddress</a> to a vertex and immediately fetches it into a local variable. This involves copying each of the struct members into the local <a class="el" href="struct_vertex.html">Vertex</a> instance. It's not very much data, just the ID, color, number of children, and a pointer to the children. The reason we have to do this fetch is that once we start doing recursive calls, we don't know which core this will be run on. Once we've copied the <a class="el" href="struct_vertex.html">Vertex</a> data, we check the color, and potentially count the vertex. Then we grab the <a class="el" href="class_global_address.html" title="Global address class. ">GlobalAddress</a> pointing to the first child of this vertex, and spawn tasks to search each child using <code>forall_here</code>. We've chosen to spawn "unbound" tasks so they can be load balanced, and remember we have to specify <code>async</code>, otherwise we would be nesting calls to <code><a class="el" href="group___synchronization.html#ga0e49ee2c2ae50e793e427b0fcfa842f9" title="Proxy for remote ConditionVariable manipulation. ">gce.wait()</a></code>.</p>
<h3>Alternative delegate</h3>
<p>If <a class="el" href="struct_vertex.html">Vertex</a> was a bit larger and more unwieldy, we might instead choose to do a delegate call to save some data movement. For instance, we could do:</p>
<p>```cpp void <a class="el" href="search2_8cpp.html#a02a63a33829d9b2f7a6375259af47fe9">search(GlobalAddress&lt;Vertex&gt; vertex_addr)</a> { auto pair = delegate::call(vertex_addr, [](<a class="el" href="struct_vertex.html">Vertex</a> * v){ if (v-&gt;color == search_color) count++; return make_pair(v-&gt;first_child, v-&gt;num_children); });</p>
<p>auto children = pair.first; forall_here&lt;unbound,async,&amp;gce&gt;(0, pair.second, [children](int64_t i){ search(children+i); }); } ```</p>
<p>The advantage of this is that we end up only moving half of the <code><a class="el" href="struct_vertex.html">Vertex</a></code>. As an aside, we can't do the <code>forall_here</code> inside the delegate because it requires doing a potentially-blocking call to enroll tasks with the GCE.</p>
<h3>Use a GlobalVector to save the results</h3>
<p>What if, instead of just counting the matches, we actually wanted to keep a list of them? <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a> provides a few simple data structures for cases such as this. They all work in a similar fashion, so this next example will demonstrate some of the quirks of how to use these highly useful data structures.</p>
<p>A GlobalVector is a wrapper around a global heap allocation (the underlying array), which allows tasks on any core to access, push, or pop elements, all safely synchronized and optimized for high throughput with lots of concurrent accessors. The way this is accomplished is by allocating a "proxy" structure on each core which will service requests in parallel on each core, and combine them into bulk requests, which are more efficient.</p>
<p>This time, in <code>main</code>, instead of initializing <code>count</code> to 0, we create the GlobalVector that will hold vertex indices:</p>
<p>```cpp <a class="el" href="class_global_address.html" title="Global address class. ">GlobalAddress</a>&lt;GlobalVector&lt;index_t&gt;&gt; rv = GlobalVector&lt;index_t&gt;::create(num_vertices);</p>
<p>// initialize all cores on_all_cores([=]{ search_color = 7; // arbitrary search results = rv; }); ```</p>
<p>We have to use the static <code>create</code> method rather than calling <code>new</code> because it actually allocates space on each core, initializes them all, and returns a <b>symmetric</b> <a class="el" href="class_global_address.html" title="Global address class. ">GlobalAddress</a> that resolves to the proxy on each core. We then use the same top-level variable trick as with <code>search_color</code> to make this <a class="el" href="class_global_address.html" title="Global address class. ">GlobalAddress</a> available to all of the cores.</p>
<p>Now in search, we do all the same things, except this time, if the color matches, we use the symmetric address <code>results</code> in a way we haven't seen yet:</p>
<p>```cpp // &ndash; inside <a class="el" href="search1_8cpp.html#a02a63a33829d9b2f7a6375259af47fe9">search()</a> function, see the rest in <a class="el" href="search2_8cpp.html">tutorial/search2.cpp</a> if (v.color == search_color) { // save the id to the results vector results-&gt;push( v.id ); } ```</p>
<p>The member dereference operator (<code>operator-&gt;()</code>) has been overloaded for <a class="el" href="class_global_address.html" title="Global address class. ">GlobalAddress</a> to get at the pointer for the current core. In order for this to work correctly, it has to be called <em>on the core where it is valid</em>. In the case of symmetric GlobalAddresses like this one, it is valid to do this on any core, but <em>it must be called on the core it is used on</em>. You must be careful not to extract the pointer and then pass that around, as some computation has to be done by <a class="el" href="class_global_address.html" title="Global address class. ">GlobalAddress</a> to resolve the address differently on each core. With that overloaded operator, it is then easy to call GlobalVector's <code>push</code> method with the current vertex id. This blocks until the <code>push</code> has finished, just like other delegate operations.</p>
<h2>Section 7: Further reading </h2>
<p>For more on <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a>'s design decisions, see our papers, available from the <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a> webiste.</p>
<p>To find out more about <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a>'s API, take a look at the autogenerated API docs. <code>BUILD.md</code> discusses building them for yourself, or you can examine the copy on the <a class="el" href="namespace_grappa.html" title="this core&#39;s base pointer ">Grappa</a> website.</p>
<p>Finally, here are some good examples in the repo:</p>
<ul>
<li><code>system/New_loop_tests.cpp</code></li>
<li><code>system/New_delegate_tests.cpp</code></li>
<li><code>system/CompletionEvent_tests.cpp</code></li>
<li><code>system/Collective_tests.cpp</code></li>
<li><code>applications/graph500/grappa/{main_new,bfs_local_adj,cc_new}.cpp</code></li>
<li><code>applications/NPB/GRAPPA/IS/intsort.cpp</code></li>
<li><code>applications/suite/grappa/{centrality.cpp,main.cpp}</code></li>
<li><code>applications/pagerank/pagerank.cpp</code> </li>
</ul>
</div></div><!-- contents -->
</div><!-- doc-content -->
<!-- start footer part -->
<div id="nav-path" class="navpath"><!-- id is needed for treeview function! -->
  <ul>
    <li class="footer">Generated by
    <a href="http://www.doxygen.org/index.html">
    <img class="footer" src="doxygen.png" alt="doxygen"/></a> 1.8.6 </li>
  </ul>
</div>
</body>
</html>
